{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Training your own openWakeWord models\n"
      ],
      "metadata": {
        "id": "vl_FIEj-auGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quick-start:** If you just want to train a basic custom model for openWakeWord!\n",
        "\n",
        "Follow the instructions for Step 1 below. Each time you change the wake word, click the play icon to the left of the title to generate a sample and make sure it sounds correct. The first time it takes a few minutes but subsequent runs will be quick.\n",
        "\n",
        "Once you're satisfied with the pronounciation, go to the \"Runtime\" dropdown menu in the upper left of the page, and select \"run all\". Keep the tab open but feel free to do something else. After ~1 hour, your custom model will be ready and will automatically be downloaded to your computer!\n",
        "\n",
        "If you are a Home Assistant user with the openWakeWord add-on, follow the instructions [here](https://github.com/home-assistant/addons/blob/master/openwakeword/DOCS.md#custom-wake-word-models) to install and enable your custom model.\n",
        "\n",
        "---\n",
        "\n",
        "If you are interested in learning more about the custom model training process (and increasing the accuracy of your custom models), read through each step in this notebook and try experimenting with different training parameters. If you have any questions or problems, feel free to start a discussion at the openWakeWord [repo](https://github.com/dscripka/openWakeWord/discussions)."
      ],
      "metadata": {
        "id": "-Q9wEuRdwY_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Xh5m1wiqhpvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "# @markdown # 1. Test Example Training Clip Generation\n",
        "# @markdown Since openWakeWord models are trained on synthetic examples of your\n",
        "# @markdown target wake word, it's a good idea to make sure that the examples\n",
        "# @markdown sound correct. Type in your target wake word below, and run the\n",
        "# @markdown cell to listen to it.\n",
        "# @markdown\n",
        "# @markdown Here are some tips that can help get the wake word to sound right:\n",
        "\n",
        "# @markdown - If your wake word isn't being pronounced in the way\n",
        "# @markdown you want, try spelling out the sounds phonetically with underscores\n",
        "# @markdown separating each part.\n",
        "# @markdown For example: \"hey siri\" --> \"hey_seer_e\".\n",
        "\n",
        "# @markdown - Spell out numbers (\"2\" --> \"two\")\n",
        "\n",
        "# @markdown - Avoid all punctuation except for \"?\" and \"!\", and remove unicode characters\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from IPython.display import Audio\n",
        "if not os.path.exists(\"./piper-sample-generator\"):\n",
        "    !git clone https://github.com/rhasspy/piper-sample-generator\n",
        "    !wget -O piper-sample-generator/models/en_US-libritts_r-medium.pt 'https://github.com/rhasspy/piper-sample-generator/releases/download/v2.0.0/en_US-libritts_r-medium.pt'\n",
        "\n",
        "    # Install system dependencies\n",
        "    !pip install piper-phonemize-cross\n",
        "    !pip install webrtcvad\n",
        "    !pip install 'torch<=2.5' torchvision torchaudio\n",
        "\n",
        "    if \"piper-sample-generator/\" not in sys.path:\n",
        "        sys.path.append(\"piper-sample-generator/\")\n",
        "\n",
        "    from generate_samples import generate_samples\n",
        "\n",
        "target_word = 'Juniper' # @param {type:\"string\"}\n",
        "\n",
        "def text_to_speech(text):\n",
        "    generate_samples(text = text,\n",
        "                max_samples=1,\n",
        "                length_scales=[1.1],\n",
        "                noise_scales=[0.7], noise_scale_ws = [0.7],\n",
        "                output_dir = './', batch_size=1, auto_reduce_batch_size=True,\n",
        "                file_names=[\"test_generation.wav\"]\n",
        "                )\n",
        "\n",
        "text_to_speech(target_word)\n",
        "Audio(\"test_generation.wav\", autoplay=True)\n"
      ],
      "metadata": {
        "id": "1cbqBebHXjFD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "220d7333-0b2a-4281-db6f-66cdd5b270e2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'piper-sample-generator'...\n",
            "remote: Enumerating objects: 124, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 124 (delta 42), reused 34 (delta 34), pack-reused 69 (from 1)\u001b[K\n",
            "Receiving objects: 100% (124/124), 1.03 MiB | 3.02 MiB/s, done.\n",
            "Resolving deltas: 100% (52/52), done.\n",
            "--2025-08-28 21:27:19--  https://github.com/rhasspy/piper-sample-generator/releases/download/v2.0.0/en_US-libritts_r-medium.pt\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/642029941/73f4af3c-7cf8-4547-a7b9-3bd29e7f3c33?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-08-28T22%3A04%3A47Z&rscd=attachment%3B+filename%3Den_US-libritts_r-medium.pt&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-08-28T21%3A04%3A25Z&ske=2025-08-28T22%3A04%3A47Z&sks=b&skv=2018-11-09&sig=Nt4WFq6T9vmnaBF%2FhV%2B672Efc4igLKMOY6P7IPletQw%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NjQxNjc0MCwibmJmIjoxNzU2NDE2NDQwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.BvVcPIz-SooaiAd07oAwt3e4hfnCIrsfuYElz81eCqs&response-content-disposition=attachment%3B%20filename%3Den_US-libritts_r-medium.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-08-28 21:27:20--  https://release-assets.githubusercontent.com/github-production-release-asset/642029941/73f4af3c-7cf8-4547-a7b9-3bd29e7f3c33?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-08-28T22%3A04%3A47Z&rscd=attachment%3B+filename%3Den_US-libritts_r-medium.pt&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-08-28T21%3A04%3A25Z&ske=2025-08-28T22%3A04%3A47Z&sks=b&skv=2018-11-09&sig=Nt4WFq6T9vmnaBF%2FhV%2B672Efc4igLKMOY6P7IPletQw%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NjQxNjc0MCwibmJmIjoxNzU2NDE2NDQwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.BvVcPIz-SooaiAd07oAwt3e4hfnCIrsfuYElz81eCqs&response-content-disposition=attachment%3B%20filename%3Den_US-libritts_r-medium.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 204089915 (195M) [application/octet-stream]\n",
            "Saving to: ‘piper-sample-generator/models/en_US-libritts_r-medium.pt’\n",
            "\n",
            "piper-sample-genera 100%[===================>] 194.63M   139MB/s    in 1.4s    \n",
            "\n",
            "2025-08-28 21:27:21 (139 MB/s) - ‘piper-sample-generator/models/en_US-libritts_r-medium.pt’ saved [204089915/204089915]\n",
            "\n",
            "Collecting piper-phonemize-cross\n",
            "  Downloading piper_phonemize_cross-1.2.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (325 bytes)\n",
            "Downloading piper_phonemize_cross-1.2.1-cp312-cp312-manylinux_2_28_x86_64.whl (15.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.6/15.6 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: piper-phonemize-cross\n",
            "Successfully installed piper-phonemize-cross-1.2.1\n",
            "Collecting webrtcvad\n",
            "  Downloading webrtcvad-2.0.10.tar.gz (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: webrtcvad\n",
            "  Building wheel for webrtcvad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for webrtcvad: filename=webrtcvad-2.0.10-cp312-cp312-linux_x86_64.whl size=73523 sha256=8152357ebfeff2b3678ee228a287bc9abfd9eaa9ac6ceb4303d88071f246d522\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/d3/95/680fa3b16848f1a58d2edaed34c496224c89a9bc63e17b3614\n",
            "Successfully built webrtcvad\n",
            "Installing collected packages: webrtcvad\n",
            "Successfully installed webrtcvad-2.0.10\n",
            "Collecting torch<=2.5\n",
            "  Downloading torch-2.5.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<=2.5) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch<=2.5) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<=2.5) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.5) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<=2.5) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<=2.5)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<=2.5)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<=2.5)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<=2.5)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<=2.5)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<=2.5)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<=2.5)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<=2.5)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<=2.5)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch<=2.5)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch<=2.5)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<=2.5)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch<=2.5)\n",
            "  Downloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<=2.5) (75.2.0)\n",
            "Collecting sympy==1.13.1 (from torch<=2.5)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch<=2.5) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.22.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.20.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
            "  Downloading torchaudio-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading torchaudio-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading torchaudio-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading torchaudio-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Downloading torchaudio-2.5.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<=2.5) (3.0.2)\n",
            "Downloading torch-2.5.0-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m770.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.20.0-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.5.0-cp312-cp312-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1747798428.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install piper-phonemize-cross'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install webrtcvad'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pip install 'torch<=2.5' torchvision torchaudio\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"piper-sample-generator/\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mmetadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    450\u001b[0m         )\n\u001b[1;32m    451\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_adapters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/_adapters.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_headers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_repair_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# suppress spurious error from mypy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/_adapters.py\u001b[0m in \u001b[0;36m_repair_headers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtextwrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdedent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m8\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mredent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_headers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_payload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Description'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/_adapters.py\u001b[0m in \u001b[0;36mredent\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repair_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mredent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;34m\"Correct for RFC822 indentation\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'\\n'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "# @markdown # 2. Download Data\n",
        "# @markdown Training custom models requires downloading a wide variety of data\n",
        "# @markdown that will help make the model perform well in real-world scenarios.\n",
        "# @markdown This example notebook will download small samples of background noise,\n",
        "# @markdown music, and Room Impulse Responses (to add echo). This will still produce\n",
        "# @markdown a custom model that performs well, but if you are interested in adding even more,\n",
        "# @markdown feel free to extend this notebook to download the full datasets and even add\n",
        "# @markdown your own!\n",
        "# @markdown\n",
        "# @markdown Downloading this example data will usually take about 15 minutes.\n",
        "\n",
        "# @markdown **Important note!** The data downloaded here has a mixture of different\n",
        "# @markdown licenses and usage restrictions. As such, any custom models trained with this\n",
        "# @markdown data should be considered as appropriate for **non-commercial** personal use only.\n",
        "\n",
        "# ## Install all dependencies\n",
        "# !pip install datasets\n",
        "# !pip install scipy\n",
        "# !pip install tqdm\n",
        "\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "# install openwakeword (full installation to support training)\n",
        "!git clone https://github.com/dscripka/openwakeword\n",
        "!pip install -e ./openwakeword --no-deps\n",
        "# !cd openwakeword\n",
        "\n",
        "# install other dependencies\n",
        "!pip install mutagen==1.47.0\n",
        "!pip install torchinfo==1.8.0\n",
        "!pip install torchmetrics==1.2.0\n",
        "!pip install speechbrain==0.5.14\n",
        "!pip install audiomentations==0.33.0\n",
        "!pip install torch-audiomentations==0.11.0\n",
        "!pip install acoustics==0.2.6\n",
        "# !pip uninstall tensorflow -y\n",
        "# !pip install tensorflow-cpu==2.8.1\n",
        "# !pip install protobuf==3.20.3\n",
        "# !pip install tensorflow_probability==0.16.0\n",
        "# !pip install onnx_tf==1.10.0\n",
        "!pip install onnxruntime==1.22.1 ai_edge_litert==1.4.0 onnxsim\n",
        "!pip install onnx2tf\n",
        "!pip install onnx\n",
        "# !pip install ai_edge_litert==1.2.0\n",
        "!pip install onnx_graphsurgeon\n",
        "!pip install sng4onnx\n",
        "!pip install pronouncing==0.2.0\n",
        "!pip install librosa soundfile\n",
        "!pip install datasets==2.14.6\n",
        "!pip install deep-phonemizer==0.0.19\n",
        "\n",
        "# Download required models (workaround for Colab)\n",
        "import os\n",
        "os.makedirs(\"./openwakeword/openwakeword/resources/models\", exist_ok=True)\n",
        "!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.onnx -O ./openwakeword/openwakeword/resources/models/embedding_model.onnx\n",
        "!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.tflite -O ./openwakeword/openwakeword/resources/models/embedding_model.tflite\n",
        "!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.onnx -O ./openwakeword/openwakeword/resources/models/melspectrogram.onnx\n",
        "!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.tflite -O ./openwakeword/openwakeword/resources/models/melspectrogram.tflite\n",
        "\n",
        "# Imports\n",
        "import sys\n",
        "\n",
        "if \"piper-sample-generator/\" not in sys.path:\n",
        "    sys.path.append(\"piper-sample-generator/\")\n",
        "from generate_samples import generate_samples\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import uuid\n",
        "import yaml\n",
        "import datasets\n",
        "import scipy\n",
        "from tqdm import tqdm\n",
        "\n",
        "## Download all data\n",
        "\n",
        "## Download MIR RIR data (takes about ~2 minutes)\n",
        "output_dir = \"./mit_rirs\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "    !git lfs install\n",
        "    !git clone https://huggingface.co/datasets/davidscripka/MIT_environmental_impulse_responses\n",
        "    rir_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(\"./MIT_environmental_impulse_responses/16khz\").glob(\"*.wav\")]}).cast_column(\"audio\", datasets.Audio())\n",
        "    # Save clips to 16-bit PCM wav files\n",
        "    for row in tqdm(rir_dataset):\n",
        "        name = row['audio']['path'].split('/')[-1]\n",
        "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
        "\n",
        "## Download noise and background audio (takes about ~3 minutes)\n",
        "\n",
        "# Audioset Dataset (https://research.google.com/audioset/dataset/index.html)\n",
        "# Download one part of the audioset .tar files, extract, and convert to 16khz\n",
        "# For full-scale training, it's recommended to download the entire dataset from\n",
        "# https://huggingface.co/datasets/agkphysics/AudioSet, and\n",
        "# even potentially combine it with other background noise datasets (e.g., FSD50k, Freesound, etc.)\n",
        "\n",
        "if not os.path.exists(\"audioset\"):\n",
        "    os.mkdir(\"audioset\")\n",
        "\n",
        "    fname = \"bal_train09.tar\"\n",
        "    out_dir = f\"audioset/{fname}\"\n",
        "    link = \"https://huggingface.co/datasets/agkphysics/AudioSet/resolve/main/data/\" + fname\n",
        "    !wget -O {out_dir} {link}\n",
        "    !cd audioset && tar -xvf bal_train09.tar\n",
        "\n",
        "    output_dir = \"./audioset_16k\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.mkdir(output_dir)\n",
        "\n",
        "    # Save clips to 16-bit PCM wav files\n",
        "    audioset_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(\"audioset/audio\").glob(\"**/*.flac\")]})\n",
        "    audioset_dataset = audioset_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
        "    for row in tqdm(audioset_dataset):\n",
        "        name = row['audio']['path'].split('/')[-1].replace(\".flac\", \".wav\")\n",
        "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
        "\n",
        "# Free Music Archive dataset\n",
        "# https://github.com/mdeff/fma\n",
        "\n",
        "output_dir = \"./fma\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "    fma_dataset = datasets.load_dataset(\"rudraml/fma\", name=\"small\", split=\"train\", streaming=True)\n",
        "    fma_dataset = iter(fma_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000)))\n",
        "\n",
        "    # Save clips to 16-bit PCM wav files\n",
        "    n_hours = 1  # use only 1 hour of clips for this example notebook, recommend increasing for full-scale training\n",
        "    for i in tqdm(range(n_hours*3600//30)):  # this works because the FMA dataset is all 30 second clips\n",
        "        row = next(fma_dataset)\n",
        "        name = row['audio']['path'].split('/')[-1].replace(\".mp3\", \".wav\")\n",
        "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
        "        i += 1\n",
        "        if i == n_hours*3600//30:\n",
        "            break\n",
        "\n",
        "\n",
        "## Android-degraded audio samples for background wake word detection\n",
        "\n",
        "\n",
        "  def android_complete_pipeline(audio_data, sample_rate=16000):\n",
        "      \"\"\"Apply complete Android background pipeline: degrade -> enhance\"\"\"\n",
        "\n",
        "      # Step 1: Apply Android background degradation\n",
        "      degraded = audio_data / 27.0  # 27x amplitude reduction\n",
        "\n",
        "      # Android AGC simulation\n",
        "      rms = np.sqrt(np.mean(degraded**2))\n",
        "      if rms > 0:\n",
        "          target_rms = 0.1\n",
        "          agc_gain = target_rms / rms\n",
        "          degraded = degraded * min(agc_gain, 4.0)\n",
        "\n",
        "      # Android audio processing effects\n",
        "      degraded = librosa.effects.preemphasis(degraded)\n",
        "      noise_level = np.random.uniform(0.001, 0.003)\n",
        "      noise = np.random.normal(0, noise_level, len(degraded))\n",
        "      degraded = degraded + noise\n",
        "      degraded = np.clip(degraded, -0.8, 0.8)  # Android compression\n",
        "\n",
        "      # Step 2: Apply your app's enhancement (OpenWakeWordEngine.kt:258-261)\n",
        "      # Convert to int16 range first (simulate AudioRecord output)\n",
        "      int16_audio = (degraded * 32767).astype(np.int16)\n",
        "\n",
        "      # Then apply your app's normalization: audioData[i].toFloat() / 32768.0f\n",
        "      enhanced_audio = int16_audio.astype(np.float32) / 32768.0\n",
        "\n",
        "      return enhanced_audio\n",
        "\n",
        "  # Update the sample generation function to use complete pipeline\n",
        "  def create_android_enhanced_samples(target_word, n_samples=500):\n",
        "      \"\"\"Generate Android complete pipeline samples\"\"\"\n",
        "      enhanced_dir = \"./android_enhanced_samples\"\n",
        "      os.makedirs(enhanced_dir, exist_ok=True)\n",
        "\n",
        "      print(f\"Generating {n_samples} Android complete pipeline samples...\")\n",
        "\n",
        "      for i in tqdm(range(n_samples)):\n",
        "          # Generate clean sample\n",
        "          generate_samples(\n",
        "              text=target_word,\n",
        "              max_samples=1,\n",
        "              length_scales=[np.random.uniform(0.9, 1.2)],\n",
        "              noise_scales=[np.random.uniform(0.5, 0.9)],\n",
        "              noise_scale_ws=[np.random.uniform(0.5, 0.9)],\n",
        "              output_dir=enhanced_dir,\n",
        "              batch_size=1,\n",
        "              auto_reduce_batch_size=True,\n",
        "              file_names=[f\"android_enhanced_{i:04d}.wav\"]\n",
        "          )\n",
        "\n",
        "          # Load, process through complete pipeline, and save\n",
        "          audio, sr = librosa.load(f\"{enhanced_dir}/android_enhanced_{i:04d}.wav\",\n",
        "  sr=16000)\n",
        "          enhanced_audio = android_complete_pipeline(audio, sr)\n",
        "          sf.write(f\"{enhanced_dir}/android_enhanced_{i:04d}.wav\", enhanced_audio, sr)\n",
        "\n",
        "  # Generate complete pipeline samples\n",
        "  create_android_enhanced_samples(target_word, n_samples=500)\n",
        "\n",
        "\n",
        "# Download pre-computed openWakeWord features for training and validation\n",
        "\n",
        "# training set (~2,000 hours from the ACAV100M Dataset)\n",
        "# See https://huggingface.co/datasets/davidscripka/openwakeword_features for more information\n",
        "if not os.path.exists(\"./openwakeword_features_ACAV100M_2000_hrs_16bit.npy\"):\n",
        "    !wget https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/openwakeword_features_ACAV100M_2000_hrs_16bit.npy\n",
        "\n",
        "# validation set for false positive rate estimation (~11 hours)\n",
        "if not os.path.exists(\"validation_set_features.npy\"):\n",
        "    !wget https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/validation_set_features.npy\n"
      ],
      "metadata": {
        "id": "oWahyFO20_mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "# @markdown # 3. Train the Model\n",
        "# @markdown Now that you have verified your target wake word and downloaded the data,\n",
        "# @markdown the last step is to adjust the training paramaters (or keep\n",
        "# @markdown the defaults below) and start the training!\n",
        "\n",
        "# @markdown Each paramater controls a different aspect of training:\n",
        "# @markdown - `number_of_examples` controls how many examples of your wakeword\n",
        "# @markdown are generated. The default (1,000) usually produces a good model,\n",
        "# @markdown but between 30,000 and 50,000 is often the best.\n",
        "\n",
        "# @markdown - `number_of_training_steps` controls how long to train the model.\n",
        "# @markdown Similar to the number of examples, the default (10,000) usually works well\n",
        "# @markdown but training longer usually helps.\n",
        "\n",
        "# @markdown - `false_activation_penalty` controls how strongly false activations\n",
        "# @markdown are penalized during the training process. Higher values can make the model\n",
        "# @markdown much less likely to activate when it shouldn't, but may also cause it\n",
        "# @markdown to not activate when the wake word isn't spoken clearly and there is\n",
        "# @markdown background noise.\n",
        "\n",
        "# @markdown With the default values shown below,\n",
        "# @markdown this takes about 30 - 60 minutes total on the normal CPU Colab runtime.\n",
        "# @markdown If you want to train on more examples or train for longer,\n",
        "# @markdown try changing the runtime type to a GPU to significantly speedup\n",
        "# @markdown the example generating and model training.\n",
        "\n",
        "# @markdown When the model finishes training, you can navigate to the `my_custom_model` folder\n",
        "# @markdown in the file browser on the left (click on the folder icon), and download\n",
        "# @markdown the [your target wake word].onnx or  <your target wake word>.tflite files.\n",
        "# @markdown You can then use these as you would any other openWakeWord model!\n",
        "\n",
        "# Load default YAML config file for training\n",
        "import yaml\n",
        "config = yaml.load(open(\"openwakeword/examples/custom_model.yml\", 'r').read(), yaml.Loader)\n",
        "\n",
        "# Modify values in the config and save a new version\n",
        "number_of_examples = 3000 # @param {type:\"slider\", min:100, max:50000, step:50}\n",
        "number_of_training_steps = 10000  # @param {type:\"slider\", min:0, max:50000, step:100}\n",
        "false_activation_penalty = 1500  # @param {type:\"slider\", min:100, max:5000, step:50}\n",
        "config[\"target_phrase\"] = [target_word]\n",
        "config[\"model_name\"] = config[\"target_phrase\"][0].replace(\" \", \"_\")\n",
        "config[\"n_samples\"] = number_of_examples + 500\n",
        "config[\"n_samples_val\"] = max(500, number_of_examples//10)\n",
        "config[\"steps\"] = number_of_training_steps\n",
        "config[\"target_accuracy\"] = 0.5\n",
        "config[\"target_recall\"] = 0.25\n",
        "config[\"output_dir\"] = \"./my_custom_model\"\n",
        "config[\"max_negative_weight\"] = false_activation_penalty\n",
        "\n",
        "config[\"background_paths\"] = ['./audioset_16k', './fma']  # multiple background datasets are supported\n",
        "config[\"target_phrase_paths\"] = ['./android_enhanced_samples']  # Use Android-degraded samples\n",
        "config[\"false_positive_validation_data_path\"] = \"validation_set_features.npy\"\n",
        "config[\"feature_data_files\"] = {\"ACAV100M_sample\": \"openwakeword_features_ACAV100M_2000_hrs_16bit.npy\"}\n",
        "\n",
        "with open('my_model.yaml', 'w') as file:\n",
        "    documents = yaml.dump(config, file)\n",
        "\n",
        "# Generate clips\n",
        "!{sys.executable} openwakeword/openwakeword/train.py --training_config my_model.yaml --generate_clips\n",
        "\n",
        "# Step 2: Augment the generated clips\n",
        "\n",
        "!{sys.executable} openwakeword/openwakeword/train.py --training_config my_model.yaml --augment_clips\n",
        "\n",
        "# Step 3: Train model\n",
        "\n",
        "!{sys.executable} openwakeword/openwakeword/train.py --training_config my_model.yaml --train_model\n",
        "\n",
        "# # Manually save to tflite as this doesn't work right in colab (broken in python 3.11, default in Colab as of January 2025)\n",
        "# def convert_onnx_to_tflite(onnx_model_path, output_path):\n",
        "#     \"\"\"Converts an ONNX version of an openwakeword model to the Tensorflow tflite format.\"\"\"\n",
        "#     # imports\n",
        "#     import onnx\n",
        "#     import logging\n",
        "#     import tempfile\n",
        "#     from onnx_tf.backend import prepare\n",
        "#     import tensorflow as tf\n",
        "\n",
        "#     # Convert to tflite from onnx model\n",
        "#     onnx_model = onnx.load(onnx_model_path)\n",
        "#     tf_rep = prepare(onnx_model, device=\"CPU\")\n",
        "#     with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "#         tf_rep.export_graph(os.path.join(tmp_dir, \"tf_model\"))\n",
        "#         converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(tmp_dir, \"tf_model\"))\n",
        "#         tflite_model = converter.convert()\n",
        "\n",
        "#         logging.info(f\"####\\nSaving tflite mode to '{output_path}'\")\n",
        "#         with open(output_path, 'wb') as f:\n",
        "#             f.write(tflite_model)\n",
        "\n",
        "#     return None\n",
        "\n",
        "# convert_onnx_to_tflite(f\"my_custom_model/{config['model_name']}.onnx\", f\"my_custom_model/{config['model_name']}.tflite\")\n",
        "\n",
        "# Convert ONNX model to tflite using `onnx2tf` library (works for python 3.11 as of January 2025)\n",
        "onnx_model_path = f\"my_custom_model/{config['model_name']}.onnx\"\n",
        "name1, name2 = f\"my_custom_model/{config['model_name']}_float32.tflite\", f\"my_custom_model/{config['model_name']}.tflite\"\n",
        "!onnx2tf -i {onnx_model_path} -o my_custom_model/ -kat onnx____Flatten_0\n",
        "!mv {name1} {name2}\n",
        "\n",
        "# Automatically download the trained model files\n",
        "from google.colab import files\n",
        "\n",
        "files.download(f\"my_custom_model/{config['model_name']}.onnx\")\n",
        "files.download(f\"my_custom_model/{config['model_name']}.tflite\")\n"
      ],
      "metadata": {
        "id": "qgaKWIY6WlJ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}